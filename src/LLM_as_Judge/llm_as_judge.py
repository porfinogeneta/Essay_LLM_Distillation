import gc
import time
import torch
from openai import OpenAI
import os
import re
from transformers import Idefics3ForConditionalGeneration, AutoProcessor
from Logger.logger import setup_logger
import json

from prompts.judge.judge_decide import prompt as decide_prompt

logger = setup_logger()

class LLmAsJudge():
    """
        Class gets test_dataset
        1. Goes through each element in the dataset
            - generates using standard SmolVLM
            - generate using fine-tuned SmolVLM
        2. There is an inependent judge - MiniMax model
        3. Firstly it checks if the fine-tuned answer is more or less the same, gives binary score 0,1
        4. Secondly it compares output from SmolVLM and fine-tuned version with the golden sample, 
        then assigns certain score.
        5. Lastly BLEU score is utilised to compare golden standard answer with the one generated by SmolVLM
    """
    def __init__(self, test_dataset, datasets_path):
        self.test_dataset = test_dataset
        self.client = OpenAI(
            api_key=os.getenv('API_KEY'),
            base_url="https://api.minimaxi.chat/v1",
        )
        self.datasets_path = datasets_path



    def clear_memory(self):
        # Delete variables if they exist in the current global scope
        if 'inputs' in globals(): del globals()['inputs']
        if 'model' in globals(): del globals()['model']
        if 'processor' in globals(): del globals()['processor']
        if 'trainer' in globals(): del globals()['trainer']
        if 'peft_model' in globals(): del globals()['peft_model']
        if 'bnb_config' in globals(): del globals()['bnb_config']
        time.sleep(2)

        # Garbage collection and clearing CUDA memory
        gc.collect()
        time.sleep(2)
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        time.sleep(2)
        gc.collect()
        time.sleep(2)

        print(f"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")

    def get_model_and_processor(self):
        model_id = "HuggingFaceTB/SmolVLM-Instruct"
        model = Idefics3ForConditionalGeneration.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            # _attn_implementation="flash_attention_2",
        )

        processor = AutoProcessor.from_pretrained(model_id)

        return model, processor
    
        

    def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device="cuda"):
        # Prepare the text input by applying the chat template
        text_input = processor.apply_chat_template(
            sample[1:2],  # Use the sample without the system message
            add_generation_prompt=True
        )

        image_inputs = []
        image = sample[1]['content'][0]['image']
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image_inputs.append([image])

        # Prepare the inputs for the model
        model_inputs = processor(
            text=text_input,
            images=image_inputs,
            return_tensors="pt",
        ).to(device)  # Move inputs to the specified device

        # Generate text with the model
        generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)

        # Trim the generated ids to remove the input ids
        trimmed_generated_ids = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        # Decode the output text
        output_text = processor.batch_decode(
            trimmed_generated_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )

        return output_text[0]  # Return the first decoded output text
    

    def generate_SmolVLM(self, sample):
        try:
            self.clear_memory()
            model, processor = self.get_model_and_processor()
            return self.generate_text_from_sample(model, processor, sample)
        except Exception as e:
            logger.error(e)

    def generate_SmolVLM_fine_tuned(self, sample):
        try: 
            self.clear_memory()
            model, processor = self.get_model_smol_processor()
            # trained adapter, we use it to inject model with new knowledge, without
            # disturbing core parameters
            adapter = "/root/Essay_LLM_Distillation/src/SmolVLM_training/smolvlm-instruct-trl-sft-ChartQA"
            model.load_adapter(adapter)
            return self.generate_text_from_sample(model, processor, sample)
        except Exception as e:
            logger.error(e)

    def extract_essay_text(self, text):
        # Split the text into sections
        sections = text.split('\n')
        essay_parts = []
        
        for line in sections:
            # Skip lines that are section headers or empty
            if line.strip().startswith(('1.', '2.', '3.')) or not line.strip():
                continue
                
            # Skip lines that are just titles in bold or contain only structural elements
            if line.strip().startswith('**') or line.strip() == 'Paragraph 1:' or \
            line.strip() == 'Paragraph 2:' or line.strip() == 'Paragraph 3:':
                continue
                
            # Remove quotation marks and clean up the text
            cleaned_line = line.strip().strip('"')
            
            # Skip if the line is empty after cleaning
            if not cleaned_line:
                continue
                
            # Add the cleaned line to our essay parts
            essay_parts.append(cleaned_line)
        
        # Join all parts with proper spacing
        essay = '\n\n'.join(essay_parts)
        
        return essay
    
    def find_ranking(text):
        match = re.search(r'(first|second)', text.lower())
        return match.group(1) if match else None


    def judge_sample_MiniMax(self, sample):

        try:
            filename = sample['filename']
            essay = sample['text']
            image = sample['image']
            title = sample['text']

            first_essay = self.extract_essay_text(self.generate_SmolVLM(sample))
            second_essay = self.extract_essay_text(self.generate_SmolVLM_fine_tuned(sample))


            smol_path = os.path.join(self.datasets_path, f"/standard/{filename}.txt")
            smol_finetuned_path = os.path.join(self.datasets_path, f"/finetuned/{filename}.txt")

            with open(smol_path, 'w') as file:
                file.write(first_essay)

            with open(smol_finetuned_path, 'w') as file:
                file.write(second_essay)

            prompt = decide_prompt.format(golden_title=title, golden_essay=essay, first_essay=first_essay, second_essay=second_essay)

            messages = [
                {
                    "role": "system",
                    "content": "You are an intelligent assistent whose role is to compare two essays. You should decide which would be better on IELTS exam and features higher score band."
                },
                {
                    "role": "user",
                    "name": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image}"
                            }
                        }
                    ]
                }
            ]

            completion = self.client.chat.completions.create(
                model="MiniMax-Text-01",
                messages=messages,
                max_tokens=100,
                )
            
            return completion.choices[0].message.content
        
        except Exception as e:
            logger.error(e)


    def judge_smolVLM(self):

        scores = {
            "first": 0,
            "second": 0
        }

        try:
            with open(f"{self.datasets_path}/test_set.json", 'r') as f:
                loaded_dataset = json.load(f)

            for sample in loaded_dataset:
                response = self.judge_sample_MiniMax(sample)
                if response == "first":
                    scores[response] += 1
                elif response == "second":
                    scores[response] += 1
                logger.warning(f"{sample}")

        except Exception as e:
            logger.error(e)

        
        return scores

if __name__ == "__main__":
    judge = LLmAsJudge(None)
    logger.debug(judge.extract_essay_text("""1. **Graph description**:
"The bar chart illustrates the number of plastic surgeons in various countries in 2019. The data is presented in descending order, with the United States having the highest number of plastic surgeons at 6,900, followed by Brazil with 6,011. The countries with the lowest numbers are Venezuela and Peru, with 625 and 563 surgeons respectively."

2. **Body paragraphs**:

Paragraph 1:
"The United States leads significantly in the number of plastic surgeons, boasting 6,900 practitioners, which is nearly 900 more than Brazil, the country with the second-highest number. China and Japan also feature prominently, with 3,000 and 2,707 surgeons respectively. South Korea, known for its advanced cosmetic procedures, has 2,571 plastic surgeons."

Paragraph 2:
"European countries such as Germany, Italy, and France have moderate numbers, with Germany having 1,397 surgeons, Italy 1,390, and France 1,082. The United Kingdom, slightly lower, has 1,077 plastic surgeons. In contrast, countries like Colombia and Argentina have fewer, with 1,130 and 1,100 surgeons respectively."

Paragraph 3:
"Among the countries with the lowest numbers are Venezuela and Peru, with 625 and 563 plastic surgeons respectively. This stark contrast highlights the disparity in access to cosmetic surgery services across different regions."

3. **Overview of the data**:
"Overall, the data reveals a significant disparity in the distribution of plastic surgeons globally. The United States and Brazil dominate the list, while many countries, particularly in South America and Asia, have considerably fewer practitioners. This disparity may reflect differences in healthcare infrastructure, economic factors, and cultural attitudes towards cosmetic surgery." """))