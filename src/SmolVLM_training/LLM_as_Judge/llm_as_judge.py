import gc
import time
import torch
from openai import OpenAI
import os
import re
from transformers import Idefics3ForConditionalGeneration, AutoProcessor
from ..utils import load_data_huggingface, format_data
from ...Logger.logger import setup_logger
import json
from tqdm import tqdm
from ...prompts.judge.judge_decide import prompt as decide_prompt

logger = setup_logger()


class LLmAsJudge():
    """
        Class gets test_dataset
        1. Goes through each element in the dataset
            - generates using standard SmolVLM
            - generate using fine-tuned SmolVLM
        2. There is an inependent judge - MiniMax model
        3. Firstly it checks if the fine-tuned answer is more or less the same, gives binary score 0,1
        4. Secondly it compares output from SmolVLM and fine-tuned version with the golden sample, 
        then assigns certain score.
        5. Lastly BLEU score is utilised to compare golden standard answer with the one generated by SmolVLM
    """
    def __init__(self, essays_path):
        self.client = OpenAI(
            api_key=os.getenv('API_KEY'),
            base_url="https://api.minimaxi.chat/v1",
        )
        self.essays_path = essays_path



    def clear_memory(self):
        # Delete variables if they exist in the current global scope
        if 'inputs' in globals(): del globals()['inputs']
        if 'model' in globals(): del globals()['model']
        if 'processor' in globals(): del globals()['processor']
        if 'trainer' in globals(): del globals()['trainer']
        if 'peft_model' in globals(): del globals()['peft_model']
        if 'bnb_config' in globals(): del globals()['bnb_config']
        time.sleep(2)

        # Garbage collection and clearing CUDA memory
        gc.collect()
        time.sleep(2)
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        time.sleep(2)
        gc.collect()
        time.sleep(2)

        # print(f"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        # print(f"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")

    def get_model_and_processor(self):
        model_id = "HuggingFaceTB/SmolVLM-Instruct"
        model = Idefics3ForConditionalGeneration.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            # _attn_implementation="flash_attention_2",
        )

        processor = AutoProcessor.from_pretrained(model_id)

        return model, processor
    
        

    def generate_text_from_sample(self, model, processor, sample, max_new_tokens=1024, device="cuda"):
        # Prepare the text input by applying the chat template
        text_input = processor.apply_chat_template(
            sample,  # Use the sample without the system message
            add_generation_prompt=True
        )

        image_inputs = []
        image = sample[1]['content'][0]['image']
        if image.mode != 'RGB':
            image = image.convert('RGB')
        image_inputs.append([image])

        # Prepare the inputs for the model
        model_inputs = processor(
            text=text_input,
            images=image_inputs,
            return_tensors="pt",
        ).to(device)  # Move inputs to the specified device

        # Generate text with the model
        generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)

        # Trim the generated ids to remove the input ids
        trimmed_generated_ids = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)
        ]

        # Decode the output text
        output_text = processor.batch_decode(
            trimmed_generated_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )

        return output_text[0]  # Return the first decoded output text
    

    def generate_SmolVLM(self, sample):
        
        try:
            self.clear_memory()
            model, processor = self.get_model_and_processor()

            
            return self.generate_text_from_sample(model, processor, sample)
        except Exception as e:
            logger.error(e)

    def generate_SmolVLM_fine_tuned(self, sample):
        try: 
            self.clear_memory()
            model, processor = self.get_model_smol_processor()
            # trained adapter, we use it to inject model with new knowledge, without
            # disturbing core parameters
            adapter = "/root/Essay_LLM_Distillation/src/SmolVLM_training/smolvlm-instruct-trl-sft-ChartQA"
            model.load_adapter(adapter)
            return self.generate_text_from_sample(model, processor, sample)
        except Exception as e:
            logger.error(e)

    def extract_essay_text(self, text):
        # Split the text into sections
        sections = text.split('\n')
        essay_parts = []
        
        for line in sections:
            # Skip lines that are section headers or empty
            if line.strip().startswith(('1.', '2.', '3.')) or not line.strip():
                continue
                
            # Skip lines that are just titles in bold or contain only structural elements
            if line.strip().startswith('**') or line.strip() == 'Paragraph 1:' or \
            line.strip() == 'Paragraph 2:' or line.strip() == 'Paragraph 3:' or \
            len(line.strip()) < 40:
                continue
                
            # Remove quotation marks and clean up the text
            cleaned_line = line.strip().strip('"')
            
            # Skip if the line is empty after cleaning
            if not cleaned_line:
                continue
                
            # Add the cleaned line to our essay parts
            essay_parts.append(cleaned_line)
        
        # Join all parts with proper spacing
        essay = '\n\n'.join(essay_parts)
        
        return essay
    
    def find_ranking(text):
        match = re.search(r'(first|second)', text.lower())
        return match.group(1) if match else None
    
    """Check how close is generated text to the gold standard"""
    def judge_corespondenxe(self, sample):
        try:
            filename = sample['filename']
            essay = sample['essay']
            image = sample['image']
            title = sample['title']

            formatted_sample = format_data(sample)
            fine_tuned_essay = self.extract_essay_text(self.generate_SmolVLM_fine_tuned(formatted_sample))

        except Exception as e:
            logger.error(e)

    def judge_sample_MiniMax(self, sample):

        try:
            filename = sample['filename']
            essay = self.extract_essay_text(sample['essay'])
            image = sample['image']
            title = sample['title']

            formatted_sample = format_data(sample)

            standard_essay = self.extract_essay_text(self.generate_SmolVLM(formatted_sample))
            fine_tuned_essay = self.extract_essay_text(self.generate_SmolVLM_fine_tuned(formatted_sample))

            # golden
            golden_dir = os.path.join(self.essays_path, "golden")
            os.makedirs(standard_dir, exist_ok=True)

            # standard
            standard_dir = os.path.join(self.essays_path, "standard")
            os.makedirs(standard_dir, exist_ok=True)
            smol_path = os.path.join(standard_dir, f"{filename}.txt")

            # fine tuned
            finetuned_dir = os.path.join(self.essays_path, "finetuned")
            os.makedirs(standard_dir, exist_ok=True)
            smol_finetuned_path = os.path.join(finetuned_dir, f"/finetuned/{filename}.txt")
            
            # save judged on essays
            with open(golden_dir, 'w') as file:
                file.write(essay)

            with open(smol_path, 'w') as file:
                file.write(standard_essay)

            with open(smol_finetuned_path, 'w') as file:
                file.write(fine_tuned_essay)

            prompt = decide_prompt.format(golden_title=title, golden_essay=essay, standard_essay=standard_essay, fine_tuned_essay=fine_tuned_essay)

            messages = [
                {
                    "role": "system",
                    "content": "You are an intelligent assistent whose role is to compare two essays. You should decide which would be better on IELTS exam and features higher score band."
                },
                {
                    "role": "user",
                    "name": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image}"
                            }
                        }
                    ]
                }
            ]

            completion = self.client.chat.completions.create(
                model="MiniMax-Text-01",
                messages=messages,
                max_tokens=100,
                )
            logger.debug("completion", completion)
            return completion.choices[0].message.content
        
        except Exception as e:
            logger.error(e)


    def judge_smolVLM(self):

        scores = {
            "first": 0,
            "second": 0
            }

        try:
            _, test_dataset, _ = load_data_huggingface()

            for sample in tqdm(test_dataset):
                response = self.judge_sample_MiniMax(sample)
                if response == "first":
                    scores[response] += 1
                elif response == "second":
                    scores[response] += 1
                # else:
                #     scores["issues"].append(response)
                # logger.debug(scores)

        except Exception as e:
            logger.error(e)

        
        return scores

if __name__ == "__main__":
    judge = LLmAsJudge(essays_path="/root/Essay_LLM_Distillation/src/SmolVLM_training/LLM_as_Judge/JudgeDatasets")
    print(judge.judge_smolVLM())